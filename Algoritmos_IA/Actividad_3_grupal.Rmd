---
title: "Resolución Actividad 3 máster Bioinformática UNIR (2025)"
author: "Laura Yera Fernandez, Edurne García Vidal, Sergio Gil Peña, Ander López Imas"
date: "2025-06-10"
output:
  html_document:
    theme:
      bootswatch: flatly
    toc: TRUE
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Algoritmos e Inteligencia Artificial: Actividad 3 grupal

## Librerías

```{r warning = FALSE, message = FALSE}
rm(list=ls())
library(ggplot2)
library(stats)
library(Rtsne)
library(RDRToolbox)
library(uwot)
library(glmnet)
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(PRROC)
library(gridExtra)
library(grDevices)
library(randomForest)
library(factoextra)
library(cluster)
library(kernlab)
```

## Lectura y procesamiento de los datos

```{r}
setwd("G:/PERSONAL/Edurne/MasterBioInformatica_UNIR/Asignaturas/1rSemestre_AlgoritmoseInteligenciaArtificial/Actividad3")

genes.raw <- readLines("column_names.txt")

labels.raw <- read.csv('classes.csv', header = FALSE, sep = ";")
colnames(labels.raw) <- c("X","Class")
labels.raw$Class <- as.factor(labels.raw$Class)

data.raw <- read.csv('gene_expression.csv', header = FALSE, sep = ";")
colnames(data.raw) <- genes.raw 
#asumimos que la lista de genes corresponde ordenadamente a las columnas de los datos de la expresión de genes, si no quisieramos relacionarlo deberiamos escribir este código colnames(data.raw) <- paste0("gene_", c(1:500))
rownames(data.raw) <- labels.raw$X
data.raw$Class <- labels.raw$Class
```

## Sanity check data e imputación de datos faltantes

```{r}
anyNA(data.raw) 
#Vemos que no hay datos NA o NaN (Not available o Not a number) en la data, por 
#lo que no nos hará falta imputar.

any(data.raw[ , -ncol(data.raw)] == 0)
zero_counts <- colSums(data.raw[ , -ncol(data.raw)] == 0)
zero_counts

zero_df <- data.frame(
  Variable = names(zero_counts),
  Zeros = as.numeric(zero_counts)
)
ggplot(zero_df, aes(x = Variable, y = Zeros, fill = Variable)) +
  geom_bar(stat = "identity") +
  labs(title = "Cantidad de ceros por columna",
       x = "Variable",
       y = "Número de ceros") +
  theme_minimal() +
  theme(legend.position = "none", 
        axis.text.x = element_blank())  # Oculta la leyenda


# Vemos que hay genes que no tienen ceros, genes que tienen algún cero y genes que 
# tienen muchísimos (>600). Estos últimos casos podrían deberse o a que el gen no se
# expresa o a que hay un error de detección en la técnica. En cualquiera de los casos,
# lo mejor sería simplemente eliminar dichas variables. 

# Estableceremos la norma de eliminar aquellas variables con 75% o más de ceros.
max_zeros <- 0.75*nrow(data.raw)

table(zero_counts > max_zeros)
genes_a_eliminar <- names(zero_counts[zero_counts > max_zeros])

data_filtered <- data.raw %>%
  select(-all_of(genes_a_eliminar))

# También eliminaremos todos aquellos genes que tengan el mismo valor en todas
# las observaciones (sd == 0) ya que no aportan ninguna información.

data_filtered_sd <- sapply(data_filtered[, -ncol(data_filtered)], sd)
anyNA(data_filtered_sd)
table(data_filtered_sd == 0)

# Todos son FALSE (y no hay ningun dato NA), asi que no hay ninguna variable con SD == 0.

# Podemos hacer un diagrama de cajas para variable y vemos los estadísticos y outliers
boxplot(data_filtered[, 1:10], main = "Boxplot de los 10 primeros genes")

# Sólo mirando los primeros genes, ya vemos que se mueven en ordenes distintos, 
# por lo que lo más correcto sería escalar los datos para que puedan ser comparables.
# Si decidimos hacerlo, lo haremos directamente en los métodos de aprendizaje.
```

## Implementación de métodos no supervisados

### Reducción de dimensionalidad

A partir de métodos no supervisados reducimos la dimensionalidad de los datos, que consiste en 
transformar el conjunto de datos que tenemos de forma que se mantenga la información más relevante
y se descarte la más redundante.

Para ello, los dos métodos de reducción de dimensionalidad escogidos son: t-SNE y UMAP.

#### t-SNE

```{r}
# Seteamos la semilla para que sea replicable el algoritmo
set.seed(1999)

#Guardar en un dataframe los genes
df <- sapply(data_filtered[1:476], as.numeric)

tsne <- Rtsne(X = df, dims = 2)
tsne_result <- data.frame(tsne$Y)

# Graficamos
tsne_plot <- ggplot(tsne_result, aes(x = X1, y = X2, color = data_filtered$Class)) +
  geom_point(size = 2) +
  scale_color_manual(values = c("red", "blue", "green", "orange", "purple")) +
  labs(title = "Reducción de dimensionalidad - t-SNE", x = "Dim 1", y = "Dim 2", color = "Tipos de cancer") +
  theme_classic() +
  theme(panel.grid.major = element_line(color = "gray90"),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"),
        plot.title=element_text(hjust=0.5))

tsne_plot

# A partir del método no supervisado anterior (t-SNE) se ha mostrado una buena separación de los grupos, manteniendo
# bien la estructura local de los puntos cercanos entre sí. Aún así, se puede observar algún punto desviado cercano 
# a un grupo distinto al que realmente pertenece. Por esta razón, decidimos probar otra técnica no lineal como es
# UMAP, que se usa cuando los datos son más complejos y que podrían ofrecer una representación distinta de la
# estructura de nuestros datos. Además, con esta otra técnica se podría reforzar los patrones observados con t-SNE.

```

#### UMAP

```{r}
# Seteamos la semilla para que sea replicable el algoritmo
set.seed(1999)

# Usamos el mismo dataframe (df) de los genes 
umap.results <- umap(df, 
                     n_neighbors = 0.1*nrow(df),
                     n_components = 2,
                     min_dist = 0.2,
                     local_connectivity=1,
                     ret_model = TRUE,
                     verbose = FALSE,
                     scale = FALSE)

umap.df <- data.frame(umap.results$embedding)

# Graficamos
umap_plot <- ggplot(umap.df, aes(x = X1, y = X2, color = data_filtered$Class)) +
  geom_point(size = 2) +
  scale_color_manual(values = c("red", "blue", "green", "orange", "purple")) +
  labs(title = "Reducción de dimensionalidad - UMAP", x = "Dimension1", y = "Dimension2", color = "Tipos de cancer") +
  theme_classic() +
  theme(panel.grid.major = element_line(color = "gray90"),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"),
        plot.title=element_text(hjust=0.5))

umap_plot

# UMAP es una técnica de reducción de dimensionalidad que asume que las muestras están distribuidas de manera
# uniforme en un espacio topológico y que se puede aproximar a partir de estas muestras para proyectarlas en 
# un espacio de menor dimensión. Al aplicar UMAP, se genera una visualización de dos dimensiones, donde podemos
# observar claramente los 5 grupos correspondientes a los 5 tipos de cancer. Pese a ser la técnica con mayor
# costo computacional, UMAP mantiene tanto la estructura local como la global, lo que la hace más últil para
# representar datos con estructuras complejas como los que tenemos.
# A pesar de ello, seguimos viendo algún punto verde (CGC) muy cerca a otros grupos (HPB y CFB), así como algún
# punto rojo que al parecer se parece mucho al cluster de CFB.
```

### Clusterización

Con los métodos de clusterización, lo que buscamos es crear subgrupos dentro de nuestras
muestras, de modo que aquellas más similares se juntaran. En esta asignatura hemos estudiado
dos tipos de métodos de clusterización: jerárquicos y no jerárquicos. Probaremos uno
de cada tipo.

#### Clusterización no jerárquica: método K-means

```{r}
# En el método K-means deberemos ajustar el número de centroides (k), para ello
# vamos primero a identificar cual es el número óptimo para nuestros datos.
# Pero antes aplicaremos un escalado a nuestros datos, ya que k-means usa distancias
# euclidianas.

df_scaled <- scale(df)

clusters_plot <- fviz_nbclust(df_scaled, kmeans, method = "wss") +
  ggtitle("Número óptimo de clusters") +
  theme_classic() +
  theme(panel.grid.major = element_line(color = "gray90"),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"),
        plot.title=element_text(hjust=0.5))

clusters_plot
# Siguiendo la regla del codo (escoger el punto a partir del cual la curva se 
# allana), hemos decidido escoger k = 4 como el valor óptimo.

set.seed(1999)
kmeans.result <- kmeans(df_scaled,
                        centers = 4,
                        iter.max = 100,
                        nstart = 50)

kmeans_plot <- fviz_cluster(kmeans.result, df_scaled) +
  ggtitle("Clusterización K-means (4 clusters)") +
  theme(panel.grid.major = element_line(color = "gray90"),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"),
        plot.title=element_text(hjust=0.5))

kmeans_plot

# A pesar de escalar datos y modificar el número de veces que se reinicia el algoritmo,
# sólo podemos separar un grupo, mientras que los otros tres clusters quedan muy pegados.
# Esto puede deberse a que nuestro dataset no es especialmente pequeño, y con datos
# pequeños es cuando este método es más eficiente.
```

#### Clusterización jerárquica: DIANA (Divisive Analysis Clustering)

```{r}
# Para este método también usaremos los datos escalados (como ya lo hemos hecho
# de antemano, usaremos stand = FALSE)

diana.results.euclidean <- diana(df_scaled, stand = FALSE, metric = "euclidean")
diana.results.manhattan <- diana(df_scaled, stand = FALSE, metric = "manhattan")

# Hemos probado ambas metricas y nos ha gustado más el resultado con manhattan:
diana_plot <- fviz_dend(diana.results.manhattan, cex = 0, k = 5,
                        palette = c("red", "blue", "green", "purple", "orange"),
                        main = "DIANA (Divisive Analysis Clustering) - Manhattan",
                        xlab = "Índice de Observaciones",
                        ylab = "Distancia") +
  theme(panel.grid.major = element_line(color = "gray90"),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"),
        plot.title=element_text(hjust=0.5))

diana_plot

# Hemos decidido usar un k = 5, por si coincidia con el número de grupos de nuestra
# base de datos, pero no parece que coincidan (uno de los grupos parece que tiene solo como observaciones).
# Tal vez se debería hacer una reducción de dimensionalidad previa, o trabajar con una base de
# datos más sencilla.
```

## Implementación de métodos supervisados

En esta parte, implementaremos tres métodos supervisados distintos para poder predecir 
una variable objetivo a partir de unos datos de entrada. Para ello debemos dividir 
nuestro dataframe en dos partes: una parte del dataframe para entrenar el modelo y otra 
parte para testear ese modelo.

### Filtrado de variables mediante LASSO/Ridge/Elastic Net

En primer lugar, queremos ver si hay alguno de los genes de nuestra base de datos que 
importa menos, de esta manera podemos descartar aquellos menos importantes y que la base 
de datos esté más limpia de datos innecesarios. Para ello, utilizamos modelos lineales 
de regularización, que son básicamente como modelos de regresión. Hay 3 modelos:

- Modelos Ridge: da un peso no nulo a todas las varibles (pero muy cercano para las que no aportan)
- Modelos LASSO: da un peso nulo a las variables poco importantes
- Modelos Elasticnet (mix): quita variables pero no muchas

En nuestro caso utilizamos Elasticnet, para quitar aquellos genes que sean menos importantes 
o con valor 0, pero solo los más irrelevantes.

```{r}
genes <- names(data_filtered[1:476])
genes

# Preparamos los datos para el modelo Elasticnet
x <- as.matrix(data_filtered[, genes])
y <- factor(data_filtered$Class)

# Hiperparametro alpha (0-1 -> Elasticnet)
# family: multinomial porque tenemos 5 tipos de tumor

set.seed(1999)
Elasticnet_model <- cv.glmnet(x, y, family = "multinomial", alpha = 0.5) 
selected_genes <- coef(Elasticnet_model, s = "lambda.min") # extraemos los coeficientes

selected_genes_tipos <- data.frame(
  "AGH" = as.vector(selected_genes$AGH),
  "CFB" = as.vector(selected_genes$CFB),
  "CGC" = as.vector(selected_genes$CGC),
  "CHC" = as.vector(selected_genes$CHC),
  "HPB" = as.vector(selected_genes$HPB)
)

rownames(selected_genes_tipos) <- rownames(selected_genes$AGH)

# Eliminamos la fila "(Intercept)" porque no la necesitamos
selected_genes_tipos_no_intercept <- selected_genes_tipos[rownames(selected_genes_tipos) != "(Intercept)", ]

# Filtramos genes con al menos un coeficiente distinto de 0
genes_filtrados <- selected_genes_tipos_no_intercept[apply(selected_genes_tipos_no_intercept != 0, 1, any), ]
genes_filtrados

names <- rownames(genes_filtrados)
names

# Seleccionamos la variable a predecir (Class) y las variables relevantes (names)
# a partir de nuestra base de datos original filtrada (data_filtered)
data <- data_filtered %>% 
  dplyr::select(Class, all_of(names))

write_csv(data, "~/Desktop/UNIR/1Q/Algoritmo_e_Inteligencia_Artificial/Actividad grupal/stata.csv")

# Dividiremos el conjunto de datos en conjuntos de entrenamiento y prueba

table(data$Class)

set.seed(1999)
trainIndex <- createDataPartition(data$Class, p = 0.8, list = FALSE)
data$Class <- as.factor(data$Class) #importante para la prediccion (no regresion)
trainData <- data[trainIndex,]
testData <- data[-trainIndex,]
```

### Modelo supervisado K-Nearest Neightbor (KNN)

En primer lugar escogemos el modelo K-Nearest Neighbor como primer modelo supervisado 
porque es un algoritmo bastante intuitivo, fácil de entender y aplicar.
Este modelo, clasifica una variable en función de la mayoría de sus vecinos más cercanos, 
esto lo hace adecuado para este conjunto de datos que tenemos, puesto que tenemos un 
conjunto de expresión de genes que deben clasificarse en 5 tipos de tumores distintos 
según la expresión de esas variables.

```{r}
# Para el modelo k-NN utilizamos el paquete caret
# Usamos trainData para entrenar el modelo
# Metodo de cross-validation (10 folds)
# preProceso -> Escalado y centrado de los datos
# tuneLength -> Probar con un vector de vecinos de longitud 30

set.seed(1999)
knnModel <- train(Class ~ .,
                  data = trainData,
                  method = "knn",
                  trControl = trainControl(method = "cv", number = 10),
                  preProcess = c("center", "scale"),
                  tuneLength = 30)
knnModel
plot(knnModel)
```

Para el modelo KNN se han utilizado para entrenar el algoritmo: 642 observaciones, 196 
variables centradas y escaladas. Además, el trainData se ha dividido en 10 particiones
para hacer la cross-validation (en cada iteración se usan 9 particiones para entrenar 
y 1 para validar, y se van rotando). El objetivo de este algoritmo es predecir el tipo 
de tumor (AGH, CFB, CGC, CHC, HPB), a partir de la expresión de genes teniendo en cuenta 
el algoritmo entrenado. 
Como resultado, tenemos que el modelo muestra su mejor resultado cuando k=5-11 
(aunque el modelo ha seleccionado k=11), es decir, cuando tiene en cuenta los 11 vecinos 
más cercanos para clasificar un nuevo resultado. Además, en k=11 se obtiene el mayor valor 
de precisión del algoritmo, siendo un valor de 0,9968990, indicando una precisión muy alta 
para poder clasificar los tumores en alguna de las 5 clases.

```{r}
# Realizamos predicciones en el conjunto de prueba utilizando el modelo entrenado
predictions_knn <- predict(knnModel, newdata = testData )
table(testData$Class)
table(predictions_knn)
# Si lo comparamos con los valores reales de la testData vemos que hay el mismo número
# de observaciones en cada clase a excepción de AGH y CFB, aunque sólo con esto no 
# podemos asegurar que se han clasificado correctamente.

# Para evaluar mejor el modelo calcularemos la matriz de confusión:
cnMatrix_knn <- confusionMatrix(predictions_knn, testData$Class)
cnMatrix_knn

# Haremos una tabla que nos enseñe diferentes métricas de evaluación de nuestro modelo:
metrics_knn <- data.frame(
  "Precision" = cnMatrix_knn$byClass[, "Pos Pred Value"],
  "Sensibilidad" = cnMatrix_knn$byClass[, "Sensitivity"],
  "Especificidad" = cnMatrix_knn$byClass[, "Specificity"],
  "Score_F1" = cnMatrix_knn$byClass[, "F1"]
)
metrics_knn
```
Observamos que el modelo de kNN ha clasificado correctamente a los pacientes de 
nuestro testData en un 99,37% de los casos (accuracy). De hecho, sólo ha fallado
en un paciente que ha clasificado como CFB, siendo realmente AGH.

Además, los parámetros de especificidad y sensibilidad para las diferentes clases son
perfectos (1 en tanto por 1) en todas las clases menos para CFB y AGH, debido al paciente 
mal clasificado, aunque aún así siguen teniendo valores superiores al 96%. La precisión 
(o valor predictivo positivo) es del 100% para todos menos para CFB (98.36%) debido al 
paciente mal clasificado. Y en cuanto al valor F1, que describe de forma global
el rendimiento para cada clase vemos que es también del 100% para todos excepto AGH y CFB,
aunque siguen teniendo valores muy altos (98.24% y 99.17%, respectivamente).

Viendo que las clasificaciones han sido practicamente perfectas, podríamos sospechar
de over-fitting, por lo que compararemos con otros modelos más complejos.


### Modelo supervisado SMV (Support Vector Machine) tipo kernel radial

Usaremos el método SMV de tipo kernel radial, y controlaremos la C para evitar
un over-fitting (si la C es muy grande).

```{r}
set.seed(1999)
svmModelKernel <- train(Class ~.,
                        data = trainData,
                        method = "svmRadial",
                        trControl = trainControl(method = "cv", number = 10, classProbs = TRUE),
                        preProcess = c("center", "scale"),
                        tuneLength = 15
                        ) 
svmModelKernel
plot(svmModelKernel)
```

Para este modelo se ha utilizado la misma trainData, con 10 particiones diferentes y 
el mejor rendimiento se ha obtenido en C=2, obteniendo una accuracy de 0.8958233, esto nos indica 
que el modelo es bueno para predecir el tipo de tumor, aunque la accuracy es peor que la del kNN.
No obstante, esto podría deberse a que el anterior introducía cierto over-fitting.
En el gráfico podemos observar que la accuracy tiene un pico global cuando la c está alrededor de 2,
con c menores la accuracy baja muchísimo, y con c mayores tiene valores ligeramente inferiores
pero se mantienen bastante estables (no obstante, como una C alta puede inducir a over-fitting,
es mejor quedarnos con una C baja).

```{r}
# Realizamos las predicciones:
predictions_svmRadial <- predict(svmModelKernel, newdata = testData )
table(testData$Class)
table(predictions_svmRadial)

# Observamos que aunque AGH, CFB y HPB tienen el mismo número de pacientes entre
# los datos reales y los predichos, hay un gran número de observaciones que parecen
# haberse movido de CHC a CGC.

# Evaluar la precisión del modelo utilizando la matriz de confusión
cnMatrix_svmRadial <- confusionMatrix(predictions_svmRadial, testData$Class)
cnMatrix_svmRadial

# Haremos una tabla que nos enseñe diferentes métricas de evaluación de nuestro modelo:
metrics_svmRadial <- data.frame(
  "Precision" = cnMatrix_svmRadial$byClass[, "Pos Pred Value"],
  "Sensibilidad" = cnMatrix_svmRadial$byClass[, "Sensitivity"],
  "Especificidad" = cnMatrix_svmRadial$byClass[, "Specificity"],
  "Score_F1" = cnMatrix_svmRadial$byClass[, "F1"]
)
metrics_svmRadial
```

En este modelo podemos ver que la accuracy es del 91.19%, debido a la clasificación
errónea de 13 pacientes que eran CHC y fueron clasificados como CGC. El resto de
clasificaciones fueron correctas, lo que se traduce también en valores de precisión, sensibilidad
y especificidad perfectos (100%) para las clases no afectadas, como podeoms observar en la tabla 
de metrics_svmRadial. En cambio, para CHC obtenemos una sensibilidad de menos del 50%, y para CGC
tenemos una precisión del 66,6% y una especificidad del 89.31%. En el score F1 se refleja lo mismo,
valores perfectos para las clases no afectadas por la clasificación errónea, mientras queCGC y CHC 
obtuvieron 0.80 y 0.65, respectivamente.
Parece que este modelo es especialmente bueno en clasificar correctamente tumores de las clases AGH, 
CFB, CGC y HPB, pero a los tumores de la categoría CHC los confunde la mitad de las veces con los 
tipo CGC.


### Modelo Random Forest

Ahora usaremos un modelo ensemble que combine varios árboles de decisión, para intentar
tener un buen modelo y que a la vez se reduzca el riesgo de over-fitting que tal vez
podría estar sucediendo en el kNN.

```{r}
# Usaremos settings similares al método anterior, usand trainData como datos de
# entrenamiento y una cross-validation con 10 particiones. De la misma forma,
# escalaremos y centraremos los datos antes de aplicar el método.

set.seed(1999)
rfmodel <- train(Class ~ .,
                  data = trainData,
                  method = "rf",
                  trControl = trainControl(method = "cv", number = 10),
                  preProcess = c("center", "scale"),
                  tuneLength = 10) 

rfmodel
plot(rfmodel)
```
Usando este método, el algoritmo ha decido que el mejor parámetro es mtry = 2,
con una accuracy resultante de 0.9937 y una kappa de 0.9917. Si observamos el gráfico,
vemos que a medida que aumenta el mtry, va bajando la accuracy aunque hay un máximo local
alrededor de mtry=85-90. De todos modos, la accuracy al mtry elegido sigue siendo muy buena.


```{r}
# Ahora haremos las predicciones con este modelo
predictions_rf <- predict(rfmodel, newdata = testData )
table(predictions_rf)
table(testData$Class)

# En este caso vemos que el único grupo que mantiene el número de observaciones
# entre la predicción y los valores reales es CHC, el resto tienen un valor más o menos
# predicho, por lo que a priori parece que este modelo es un poco peor al anterior.

# Para evaluar mejor modelo calcularemos la matriz de confusión:
cnMatrix_rf <- confusionMatrix(predictions_rf, testData$Class)
cnMatrix_rf

# Ahora haremos la tabla de las metricas:
metrics_rf <- data.frame(
  "Precision" = cnMatrix_rf$byClass[, "Pos Pred Value"],
  "Sensibilidad" = cnMatrix_rf$byClass[, "Sensitivity"],
  "Especificidad" = cnMatrix_rf$byClass[, "Specificity"],
  "Score_F1" = cnMatrix_rf$byClass[, "F1"]
)
metrics_rf
```
Observando la matrix de confusión, vemos que en el caso del Random Forest se ha predicho
érroneamente un paciente AGH como CFB (que ya ocurrió en el modelo kNN). Además, también 
se ha classificado de forma érronea un paciente HPB como CGC. Es por ello que, aunque las
distintas métricas siguen siendo muy elevadas (todas por encima del 90%), ahora son un poco
peores que en el modelo kNN (sobre todo las métricas asociadas a los grupos HPC y CGC, ya que
en kNN no hubo ninguna classificación errónea en esas clases).
Usando este método, la única clase en la que no ha habido ningún error de clasificación es CHC,
lo que se ve por ser la única clase con un 100% en todas las métricas.
Fijandonos en el valor F1, vemos que este método tiene un peor score para la clase HPB, aunque
probablemente sea por reducida n de observaciones (por lo que un fallo tiene un impacto mayor).


### Comparativa de los modelos de aprendizaje supervisado

Ya que el valor F1 es un valor único que nos permite evaluar cada modelo por clase,
vamos a juntar en una misma tabla los valores F1 y así poder comparar los distintos modelos.

```{r}
scores_F1 <- data.frame(
  kNN = metrics_knn$Score_F1,
  svmRadial = metrics_svmRadial$Score_F1,
  RF = metrics_rf$Score_F1
)
rownames(scores_F1) <- rownames(metrics_knn)
scores_F1
```
Como ya hemos ido viendo, al comparar los tres modelo, vemos que el mejor en cuanto
a score F1 por clase sería el modelo kNN, que tiene valores máximos para 3 de las 5 clases
(a diferencia de RF que solo tiene 1). Para aquellas clases donde el valor F1 no es máximo, 
éste sigue siendo muy elevado (0.98 y 0.99), en comparación por ejemplo a los de svmRadial 
(0.8 y 0.65).
Por lo tanto, en el caso de no producirse over-fitting, el mejor modelo sería kNN, seguido
por el Random Forest.



## Preguntas de respuesta corta

### Pregunta 1. Procesamiento de los datos.
#### ¿Qué método habéis escogido para llevar a cabo la imputación de los datos? Razonad vuestra respuesta.
La imputación de datos en el sentido de rellenar aquellos campos en los que hubiera datos NA no se ha realizado,
porque no había datos NA. En cuanto al tratamiento de 0s, se explicará a continuación.

#### ¿Habéis llevado a cabo algún otro tipo de procesamiento? Razonad vuestra respuesta.
A parte de buscar datos NA, buscamos aquellas variables que tuvieran 0s. En concreto, buscamos
variables que tuvieran una gran cantidad de 0s (en un 75% o más de las observaciones), ya que este
tipo de resultados pueden deberse a que 1) el gen no se expresa o 2) ha habido algún error en el
método de cuantificación o falta de sensibilidad. En cualquier caso, este tipo de variables probablemente 
no van a aportar mucha información que ayude a clasificar las observaciones, por lo que se han descartado
en pos de tener una base de datos lo más manejable posible. Del mismo modo, se han buscado aquellas variables
que tuvieran el mismo valor en todas las observaciones (es decir, una desviación estándard == 0), porque 
tampoco hubieran podido aportar valor clasificatorio. No hemos encontrado ninguna variable que
cumpliese ese criterio, pero de haberlo hecho, también la hubieramos eliminado.

### Pregunta 2. Métodos no supervisados.
#### ¿Cuál es el motivo por el cual habéis seleccionado estas técnicas de reducción de dimensionalidad?
Hemos decidido usar t-SNE y UMAP porque ambos pueden tratar con datos no lineales, y ambos
separan bien los grupos de forma visual (además, a partir de los ejemplos de clase y de la actividad 1 nos
parecían los mejores métodos). También son los métodos que más parecen usarse (algunos compañeros del grupo
los han visto en artículos y en sus trabajos), por lo que nos interesaba aprender a usarlos.

#### ¿Cuál es el motivo por el cual habéis seleccionado estas técnicas de clusterización?
Hemos decidido usar un método jerarquico y uno no-jerárquico. Como de los no jerárquicos
sólo hemos visto k-Means en clase, ese es el que hemos hecho. De los jerárquicos decidimos hacer 
el divisivo (DIANA), porque en teoría es menos costoso que el jerarquico algomerativo.


#### En ambos casos, ¿qué aspectos positivos y negativos tienen cada una?
- tSNE:
- UMAP:
- k-Means:
- DIANA:

#### En el caso de la clusterización, ¿podéis afirmar con certeza que los clústeres generados son 
los mejores posibles? Razonad vuestra respuesta.

### Pregunta 3. Métodos supervisados
#### ¿Cuál es el motivo por el cual habéis seleccionado ambas técnicas de aprendizaje supervisado? 
¿Cuál ha dado mejores resultados a la hora de clasificar las muestras? Razonad vuestra respuesta.

#### ¿Habéis considerado oportuno implementar algún método de reducción de dimensionalidad 
para procesar los datos antes de implementarlos en dichas técnicas? ¿Por qué?

#### ¿Qué aspectos positivos y negativos tienen cada una de las técnicas que habéis escogido?

### Pregunta 4. De estas cuatro opciones, ¿qué tipo de arquitectura de deep learning sería 
la más adecuada para procesar datos de expresión génica? Razonad vuestra respuesta.
a) Red de perceptrones (multiperceptron layers).
b) Redes convolucionales.
c) Redes recurrentes.
d) Redes de grafos.







