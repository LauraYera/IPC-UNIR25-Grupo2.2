---
title: "Resolución Actividad 3 máster Bioinformática UNIR (2025)"
author: "Laura Yera Fernandez, Edurne García Vidal, Sergio Gil Peña, Ander López Imas"
date: "2025-06-10"
output:
  html_document:
    theme:
      bootswatch: flatly
    toc: TRUE
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Algoritmos e Inteligencia Artificial: Actividad 3 grupal

## Librerías

```{r warning = FALSE, message = FALSE}
rm(list=ls())
library(tidyverse)
library(caret)
library(ggplot2)
library(stats)
library(Rtsne)
library(uwot)
library(glmnet)
library(factoextra)
library(cluster)
library(kernlab)
library(randomForest)
```

## Lectura y procesamiento de los datos

```{r}
setwd("G:/PERSONAL/Edurne/MasterBioInformatica_UNIR/Asignaturas/1rSemestre_AlgoritmoseInteligenciaArtificial/Actividad3")

genes.raw <- readLines("column_names.txt")

labels.raw <- read.csv('classes.csv', header = FALSE, sep = ";")
colnames(labels.raw) <- c("X","Class")
labels.raw$Class <- as.factor(labels.raw$Class)

data.raw <- read.csv('gene_expression.csv', header = FALSE, sep = ";")
colnames(data.raw) <- genes.raw 
#asumimos que la lista de genes corresponde ordenadamente a las columnas de los datos de la expresión de genes, si no quisieramos relacionarlo deberiamos escribir este código colnames(data.raw) <- paste0("gene_", c(1:500))
rownames(data.raw) <- labels.raw$X
data.raw$Class <- labels.raw$Class
```

## Sanity check data e imputación de datos faltantes

```{r}
anyNA(data.raw) 
#Vemos que no hay datos NA o NaN (Not available o Not a number) en la data, por 
#lo que no nos hará falta imputar.

any(data.raw[ , -ncol(data.raw)] == 0)
zero_counts <- colSums(data.raw[ , -ncol(data.raw)] == 0)
zero_counts

zero_df <- data.frame(
  Variable = names(zero_counts),
  Zeros = as.numeric(zero_counts)
)
ggplot(zero_df, aes(x = Variable, y = Zeros, fill = Variable)) +
  geom_bar(stat = "identity") +
  labs(title = "Cantidad de ceros por columna",
       x = "Variable",
       y = "Número de ceros") +
  theme_minimal() +
  theme(legend.position = "none", 
        axis.text.x = element_blank())  # Oculta la leyenda


# Vemos que hay genes que no tienen ceros, genes que tienen algún cero y genes que 
# tienen muchísimos (>600). Estos últimos casos podrían deberse o a que el gen no se
# expresa o a que hay un error de detección en la técnica. En cualquiera de los casos,
# lo mejor sería simplemente eliminar dichas variables. 

# Estableceremos la norma de eliminar aquellas variables con 75% o más de ceros.
max_zeros <- 0.75*nrow(data.raw)

table(zero_counts > max_zeros)
genes_a_eliminar <- names(zero_counts[zero_counts > max_zeros])

data_filtered <- data.raw %>%
  select(-all_of(genes_a_eliminar))

# También eliminaremos todos aquellos genes que tengan el mismo valor en todas
# las observaciones (sd == 0) ya que no aportan ninguna información.

data_filtered_sd <- sapply(data_filtered[, -ncol(data_filtered)], sd)
anyNA(data_filtered_sd)
table(data_filtered_sd == 0)

# Todos son FALSE (y no hay ningun dato NA), asi que no hay ninguna variable con SD == 0.

# Podemos hacer un diagrama de cajas para variable y vemos los estadísticos y outliers
boxplot(data_filtered[, 1:10], main = "Boxplot de los 10 primeros genes")

# Sólo mirando los primeros genes, ya vemos que se mueven en ordenes distintos, 
# por lo que lo más correcto sería escalar los datos para que puedan ser comparables.
# Si decidimos hacerlo, lo haremos directamente en los métodos de aprendizaje.
```

## Implementación de métodos no supervisados

### Reducción de dimensionalidad

A partir de métodos no supervisados reducimos la dimensionalidad de los datos, que consiste en 
transformar el conjunto de datos que tenemos de forma que se mantenga la información más relevante
y se descarte la más redundante.

Para ello, los dos métodos de reducción de dimensionalidad escogidos son: t-SNE y UMAP.

#### t-SNE

```{r}
# Seteamos la semilla para que sea replicable el algoritmo
set.seed(1999)

#Guardar en un dataframe los genes
df <- sapply(data_filtered[1:476], as.numeric)

tsne <- Rtsne(X = df, dims = 2)
tsne_result <- data.frame(tsne$Y)

# Graficamos
tsne_plot <- ggplot(tsne_result, aes(x = X1, y = X2, color = data_filtered$Class)) +
  geom_point(size = 2) +
  scale_color_manual(values = c("red", "blue", "green", "orange", "purple")) +
  labs(title = "Reducción de dimensionalidad - t-SNE", x = "Dim 1", y = "Dim 2", color = "Tipos de cancer") +
  theme_classic() +
  theme(panel.grid.major = element_line(color = "gray90"),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"),
        plot.title=element_text(hjust=0.5))

tsne_plot

# A partir del método no supervisado anterior (t-SNE) se ha mostrado una buena separación de los grupos, manteniendo
# bien la estructura local de los puntos cercanos entre sí. Aún así, se puede observar algún punto desviado cercano 
# a un grupo distinto al que realmente pertenece. Por esta razón, decidimos probar otra técnica no lineal como es
# UMAP, que se usa cuando los datos son más complejos y que podrían ofrecer una representación distinta de la
# estructura de nuestros datos. Además, con esta otra técnica se podría reforzar los patrones observados con t-SNE.

```

#### UMAP

```{r}
# Seteamos la semilla para que sea replicable el algoritmo
set.seed(1999)

# Usamos el mismo dataframe (df) de los genes 
umap.results <- umap(df, 
                     n_neighbors = 0.1*nrow(df),
                     n_components = 2,
                     min_dist = 0.2,
                     local_connectivity=1,
                     ret_model = TRUE,
                     verbose = FALSE,
                     scale = FALSE)

umap.df <- data.frame(umap.results$embedding)

# Graficamos
umap_plot <- ggplot(umap.df, aes(x = X1, y = X2, color = data_filtered$Class)) +
  geom_point(size = 2) +
  scale_color_manual(values = c("red", "blue", "green", "orange", "purple")) +
  labs(title = "Reducción de dimensionalidad - UMAP", x = "Dimension1", y = "Dimension2", color = "Tipos de cancer") +
  theme_classic() +
  theme(panel.grid.major = element_line(color = "gray90"),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"),
        plot.title=element_text(hjust=0.5))

umap_plot

# UMAP es una técnica de reducción de dimensionalidad que asume que las muestras están distribuidas de manera
# uniforme en un espacio topológico y que se puede aproximar a partir de estas muestras para proyectarlas en 
# un espacio de menor dimensión. Al aplicar UMAP, se genera una visualización de dos dimensiones, donde podemos
# observar claramente los 5 grupos correspondientes a los 5 tipos de cancer. Pese a ser la técnica con mayor
# costo computacional, UMAP mantiene tanto la estructura local como la global, lo que la hace más últil para
# representar datos con estructuras complejas como los que tenemos.
# A pesar de ello, seguimos viendo algún punto verde (CGC) muy cerca a otros grupos (HPB y CFB), así como algún
# punto rojo que al parecer se parece mucho al cluster de CFB.
```

### Clusterización

Con los métodos de clusterización, lo que buscamos es crear subgrupos dentro de nuestras
muestras, de modo que aquellas más similares se juntaran. En esta asignatura hemos estudiado
dos tipos de métodos de clusterización: jerárquicos y no jerárquicos. Probaremos uno
de cada tipo.

#### Clusterización no jerárquica: método K-means

```{r}
# En el método K-means deberemos ajustar el número de centroides (k), para ello
# vamos primero a identificar cual es el número óptimo para nuestros datos.
# Pero antes aplicaremos un escalado a nuestros datos, ya que k-means usa distancias
# euclidianas.

df_scaled <- scale(df)

clusters_plot <- fviz_nbclust(df_scaled, kmeans, method = "wss") +
  ggtitle("Número óptimo de clusters") +
  theme_classic() +
  theme(panel.grid.major = element_line(color = "gray90"),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"),
        plot.title=element_text(hjust=0.5))

clusters_plot
# Siguiendo la regla del codo (escoger el punto a partir del cual la curva se 
# allana), hemos decidido escoger k = 4 como el valor óptimo.

set.seed(1999)
kmeans.result <- kmeans(df_scaled,
                        centers = 4,
                        iter.max = 100,
                        nstart = 50)

kmeans_plot <- fviz_cluster(kmeans.result, df_scaled) +
  ggtitle("Clusterización K-means (4 clusters)") +
  theme(panel.grid.major = element_line(color = "gray90"),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"),
        plot.title=element_text(hjust=0.5))

kmeans_plot

# A pesar de escalar datos y modificar el número de veces que se reinicia el algoritmo,
# sólo podemos separar un grupo, mientras que los otros tres clusters quedan muy pegados.
# Esto puede deberse a que nuestro dataset no es especialmente pequeño, y con datos
# pequeños es cuando este método es más eficiente.
```

#### Clusterización jerárquica: DIANA (Divisive Analysis Clustering)

```{r}
# Para este método también usaremos los datos escalados (como ya lo hemos hecho
# de antemano, usaremos stand = FALSE)

diana.results.euclidean <- diana(df_scaled, stand = FALSE, metric = "euclidean")
diana.results.manhattan <- diana(df_scaled, stand = FALSE, metric = "manhattan")

# Hemos probado ambas metricas y nos ha gustado más el resultado con manhattan:
diana_plot <- fviz_dend(diana.results.manhattan, cex = 0, k = 5,
                        palette = c("red", "blue", "green", "purple", "orange"),
                        main = "DIANA (Divisive Analysis Clustering) - Manhattan",
                        xlab = "Índice de Observaciones",
                        ylab = "Distancia") +
  theme(panel.grid.major = element_line(color = "gray90"),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"),
        plot.title=element_text(hjust=0.5))

diana_plot

# Hemos decidido usar un k = 5, por si coincidia con el número de grupos de nuestra
# base de datos, pero no parece que coincidan (uno de los grupos parece que tiene solo como observaciones).
# Tal vez se debería hacer una reducción de dimensionalidad previa, o trabajar con una base de
# datos más sencilla.
```

## Implementación de métodos supervisados

En esta parte, implementaremos tres métodos supervisados distintos para poder predecir 
una variable objetivo a partir de unos datos de entrada. Para ello debemos dividir 
nuestro dataframe en dos partes: una parte del dataframe para entrenar el modelo y otra 
parte para testear ese modelo.

### Filtrado de variables mediante LASSO/Ridge/Elastic Net

En primer lugar, queremos ver si hay alguno de los genes de nuestra base de datos que 
aporte menos información y que pueda ser filtrado para que la base de datos esté más
simplificada. Para ello, utilizamos modelos lineales de regularización. Hay 3 modelos:

- Modelos LASSO: da un peso nulo a las variables poco importantes
- Modelos Ridge: da un peso pequeño pero no nulo a las variables menos importantes
- Modelos Elasticnet: es un modelo intermedio entre los dos anteriores

En nuestro caso utilizamos Elasticnet, para eliminar sólo aquellos genes más irrelevantes.

```{r}
genes <- names(data_filtered[1:476])

# Preparamos los datos para el modelo Elasticnet
x <- as.matrix(data_filtered[, genes])
y <- factor(data_filtered$Class)

# Hiperparametro alpha (0-1 -> Elasticnet)
# family: multinomial porque tenemos 5 tipos de tumor

set.seed(1999)
Elasticnet_model <- cv.glmnet(x, y, family = "multinomial", alpha = 0.5) 
selected_genes <- coef(Elasticnet_model, s = "lambda.min") # extraemos los coeficientes

selected_genes_tipos <- data.frame(
  "AGH" = as.vector(selected_genes$AGH),
  "CFB" = as.vector(selected_genes$CFB),
  "CGC" = as.vector(selected_genes$CGC),
  "CHC" = as.vector(selected_genes$CHC),
  "HPB" = as.vector(selected_genes$HPB)
)

rownames(selected_genes_tipos) <- rownames(selected_genes$AGH)

# Eliminamos la fila "(Intercept)" porque no la necesitamos
selected_genes_tipos_no_intercept <- selected_genes_tipos[rownames(selected_genes_tipos) != "(Intercept)", ]

# Filtramos genes con al menos un coeficiente distinto de 0
genes_filtrados <- selected_genes_tipos_no_intercept[apply(selected_genes_tipos_no_intercept != 0, 1, any), ]
names <- rownames(genes_filtrados)


# Seleccionamos la variable a predecir (Class) y las variables relevantes (names)
# a partir de nuestra base de datos original filtrada (data_filtered)
data <- data_filtered %>% 
  dplyr::select(Class, all_of(names))

write_csv(data, "G:/PERSONAL/Edurne/MasterBioInformatica_UNIR/Asignaturas/1rSemestre_AlgoritmoseInteligenciaArtificial/Actividad3/stata.csv")

# Dividiremos el conjunto de datos en conjuntos de entrenamiento y prueba

table(data$Class)

set.seed(1999)
trainIndex <- createDataPartition(data$Class, p = 0.8, list = FALSE)
data$Class <- as.factor(data$Class) #importante para la prediccion (no regresion)
trainData <- data[trainIndex,]
testData <- data[-trainIndex,]
```

### Modelo supervisado K-Nearest Neightbors (KNN)

En primer lugar escogemos el modelo K-Nearest Neighbors como primer modelo supervisado 
porque es un algoritmo bastante intuitivo, fácil de entender y aplicar.
Este modelo, clasifica una variable en función de la mayoría de sus vecinos más cercanos, 
esto lo hace adecuado para este conjunto de datos que tenemos, puesto que tenemos un 
conjunto de expresión de genes que deben clasificarse en 5 tipos de tumores distintos 
según la expresión de esas variables.

```{r}
# Para el modelo k-NN utilizamos el paquete caret
# Usamos trainData para entrenar el modelo
# Metodo de cross-validation (10 folds)
# preProceso -> Escalado y centrado de los datos
# tuneLength -> Probar con un vector de vecinos de longitud 30

set.seed(1999)
knnModel <- train(Class ~ .,
                  data = trainData,
                  method = "knn",
                  trControl = trainControl(method = "cv", number = 10),
                  preProcess = c("center", "scale"),
                  tuneLength = 30)
knnModel
plot(knnModel)
```

Para el modelo KNN se han utilizado para entrenar el algoritmo: 642 observaciones, 196 
variables centradas y escaladas. Además, el trainData se ha dividido en 10 particiones
para hacer la cross-validation (en cada iteración se usan 9 particiones para entrenar 
y 1 para validar, y se van rotando). El objetivo de este algoritmo es predecir el tipo 
de tumor (AGH, CFB, CGC, CHC, HPB), a partir de la expresión de genes teniendo en cuenta 
el algoritmo entrenado. 
Como resultado, tenemos que el modelo muestra su mejor resultado cuando k=5-11 
(aunque el modelo ha seleccionado k=11), es decir, cuando tiene en cuenta los 11 vecinos 
más cercanos para clasificar un nuevo resultado. Además, en k=11 se obtiene el mayor valor 
de accuracy del algoritmo, siendo un valor de 0,9968990, indicando una accuracy muy alta 
para poder clasificar los tumores en alguna de las 5 clases.

```{r}
# Realizamos predicciones en el conjunto de prueba utilizando el modelo entrenado
predictions_knn <- predict(knnModel, newdata = testData )
table(testData$Class)
table(predictions_knn)
# Si lo comparamos con los valores reales de la testData vemos que hay el mismo número
# de observaciones en cada clase a excepción de AGH y CFB, aunque sólo con esto no 
# podemos asegurar que se han clasificado correctamente.

# Para evaluar mejor el modelo calcularemos la matriz de confusión:
cnMatrix_knn <- confusionMatrix(predictions_knn, testData$Class)
cnMatrix_knn

# Haremos una tabla que nos enseñe diferentes métricas de evaluación de nuestro modelo:
metrics_knn <- data.frame(
  "Precision" = cnMatrix_knn$byClass[, "Pos Pred Value"],
  "Sensibilidad" = cnMatrix_knn$byClass[, "Sensitivity"],
  "Especificidad" = cnMatrix_knn$byClass[, "Specificity"],
  "Score_F1" = cnMatrix_knn$byClass[, "F1"]
)
metrics_knn
```
Observamos que el modelo de kNN ha clasificado correctamente a los pacientes de 
nuestro testData en un 99,37% de los casos (accuracy). De hecho, sólo ha fallado
en un paciente que ha clasificado como CFB, siendo realmente AGH.

Además, los parámetros de especificidad y sensibilidad para las diferentes clases son
perfectos (1 en tanto por 1) en todas las clases menos para CFB y AGH, debido al paciente 
mal clasificado, aunque aún así siguen teniendo valores superiores al 96%. La precisión 
(o valor predictivo positivo) es del 100% para todos menos para CFB (98.36%) debido al 
paciente mal clasificado. Y en cuanto al valor F1, que describe de forma global
el rendimiento para cada clase vemos que es también del 100% para todos excepto AGH y CFB,
aunque siguen teniendo valores muy altos (98.24% y 99.17%, respectivamente).

Viendo que las clasificaciones han sido practicamente perfectas, podríamos sospechar
de over-fitting, por lo que compararemos con otros modelos más complejos.


### Modelo supervisado SMV (Support Vector Machine) tipo kernel radial

Usaremos el método SMV de tipo kernel radial, y controlaremos la C para evitar
un over-fitting (si la C es muy grande).

```{r}
set.seed(1999)
svmModelKernel <- train(Class ~.,
                        data = trainData,
                        method = "svmRadial",
                        trControl = trainControl(method = "cv", number = 10, classProbs = TRUE),
                        preProcess = c("center", "scale"),
                        tuneLength = 15
                        ) 
svmModelKernel
plot(svmModelKernel)
```

Para este modelo se ha utilizado la misma trainData, con 10 particiones diferentes y 
el mejor rendimiento se ha obtenido en C=2, obteniendo una accuracy de 0.8958233, esto nos indica 
que el modelo es bueno para predecir el tipo de tumor, aunque la accuracy es peor que la del kNN.
No obstante, esto podría deberse a que el anterior introducía cierto over-fitting.
En el gráfico podemos observar que la accuracy tiene un pico global cuando la c está alrededor de 2,
con c menores la accuracy baja muchísimo, y con c mayores tiene valores ligeramente inferiores
pero se mantienen bastante estables (no obstante, como una C alta puede inducir a over-fitting,
es mejor quedarnos con una C baja).

```{r}
# Realizamos las predicciones:
predictions_svmRadial <- predict(svmModelKernel, newdata = testData )
table(testData$Class)
table(predictions_svmRadial)

# Observamos que aunque AGH, CFB y HPB tienen el mismo número de pacientes entre
# los datos reales y los predichos, hay un gran número de observaciones que parecen
# haberse movido de CHC a CGC.

# Evaluar la precisión del modelo utilizando la matriz de confusión
cnMatrix_svmRadial <- confusionMatrix(predictions_svmRadial, testData$Class)
cnMatrix_svmRadial

# Haremos una tabla que nos enseñe diferentes métricas de evaluación de nuestro modelo:
metrics_svmRadial <- data.frame(
  "Precision" = cnMatrix_svmRadial$byClass[, "Pos Pred Value"],
  "Sensibilidad" = cnMatrix_svmRadial$byClass[, "Sensitivity"],
  "Especificidad" = cnMatrix_svmRadial$byClass[, "Specificity"],
  "Score_F1" = cnMatrix_svmRadial$byClass[, "F1"]
)
metrics_svmRadial
```

En este modelo podemos ver que la accuracy es del 91.19%, debido a la clasificación
errónea de 13 pacientes que eran CHC y fueron clasificados como CGC. El resto de
clasificaciones fueron correctas, lo que se traduce también en valores de precisión, sensibilidad
y especificidad perfectos (100%) para las clases no afectadas, como podeoms observar en la tabla 
de metrics_svmRadial. En cambio, para CHC obtenemos una sensibilidad de menos del 50%, y para CGC
tenemos una precisión del 66,6% y una especificidad del 89.31%. En el score F1 se refleja lo mismo,
valores perfectos para las clases no afectadas por la clasificación errónea, mientras que CGC y CHC 
obtuvieron 0.80 y 0.65, respectivamente.
Parece que este modelo es especialmente bueno en clasificar correctamente tumores de las clases AGH, 
CFB, CGC y HPB, pero a los tumores de la categoría CHC los confunde la mitad de las veces con los 
tipo CGC.


### Modelo Random Forest

Ahora usaremos un modelo ensemble que combine varios árboles de decisión, para intentar
tener un buen modelo y que a la vez se reduzca el riesgo de over-fitting que tal vez
podría estar sucediendo en el kNN.

```{r}
# Usaremos settings similares al método anterior, usand trainData como datos de
# entrenamiento y una cross-validation con 10 particiones. De la misma forma,
# escalaremos y centraremos los datos antes de aplicar el método.

set.seed(1999)
rfmodel <- train(Class ~ .,
                  data = trainData,
                  method = "rf",
                  trControl = trainControl(method = "cv", number = 10),
                  preProcess = c("center", "scale"),
                  tuneLength = 10) 

rfmodel
plot(rfmodel)
```

Usando este método, el algoritmo ha decido que el mejor parámetro es mtry = 23,
con una accuracy resultante de 0.9906 y una kappa de 0.9876. Si observamos el gráfico,
vemos que al principio, la accuracy va aumentando con el mtry hasta llegar al máximo en
23, y a partir de allí aumentar el mtry sólo disminuye o estabiliza la accuracy (aunque
el valor más bajo observado en la gráfica es aún de 0.98).


```{r}
# Ahora haremos las predicciones con este modelo
predictions_rf <- predict(rfmodel, newdata = testData )
table(predictions_rf)
table(testData$Class)

# En este caso vemos los grupos AGH y CFB parecen haber intercambiado una observación,
# mientras que el resto de grupos se mantiene igual.

# Para evaluar mejor modelo calcularemos la matriz de confusión:
cnMatrix_rf <- confusionMatrix(predictions_rf, testData$Class)
cnMatrix_rf

# Ahora haremos la tabla de las metricas:
metrics_rf <- data.frame(
  "Precision" = cnMatrix_rf$byClass[, "Pos Pred Value"],
  "Sensibilidad" = cnMatrix_rf$byClass[, "Sensitivity"],
  "Especificidad" = cnMatrix_rf$byClass[, "Specificity"],
  "Score_F1" = cnMatrix_rf$byClass[, "F1"]
)
metrics_rf
```
Observando la matrix de confusión, vemos que en el caso del Random Forest se ha predicho
érroneamente un paciente AGH como CFB, y que la accuracy del modelo es de 0.9937 (valores
exactos a los del modelo kNN). Con el resto de clases no se han cometido errores, y es 
por ello que CGC, CGC y HPB tienen métricas perfectas de precision, sensibilidad, especificidad 
y score F1 (como podemos ver en la tabla metrics_rf).
Fijandonos en el valor F1, vemos que este método tiene un peor score para la AGH y CFB, por
el paciente mal clasificado, pero siguen siendo valores muy altos (los mismos que en el modelo kNN).


### Comparativa de los modelos de aprendizaje supervisado

Ya que el valor F1 es un valor único que nos permite evaluar cada modelo por clase,
vamos a juntar en una misma tabla los valores F1 y así poder comparar los distintos modelos.

```{r}
scores_F1 <- data.frame(
  kNN = metrics_knn$Score_F1,
  svmRadial = metrics_svmRadial$Score_F1,
  RF = metrics_rf$Score_F1
)
rownames(scores_F1) <- rownames(metrics_knn)
scores_F1
```
Al comparar los tres modelos vemos que los mejores, en cuanto a score F1 por clase,
serían los modelo kNN y Random Forest, ya que tienen valores perfectos para 3 de 
las cinco clases (y para las otras dos siguen teniendo valores elevados de 0.98 y 0.99).
Teniendo en cuenta que tanto el modelo kNN como el Random Forest han salido iguales, es
probable que el aprendizaje haya sido bueno y no estemos observando over-fitting.
En el modelo svmRadial vemos que el F1 score de la clase CHC es muy bajo ya que, como hemos
visto en la matriz de confusión del modelo, ha clasificado mal a la mitad de pacientes CHC.



## Preguntas de respuesta corta

### Pregunta 1. Procesamiento de los datos.
**¿Qué método habéis escogido para llevar a cabo la imputación de los datos? Razonad vuestra respuesta.**

La imputación de datos en el sentido de rellenar aquellos campos en los que hubiera datos NA no se ha realizado,
porque no había datos NA. En cuanto al tratamiento de 0s, se explicará a continuación.

**¿Habéis llevado a cabo algún otro tipo de procesamiento? Razonad vuestra respuesta.**

A parte de buscar datos NA, buscamos aquellas variables que tuvieran 0s. En concreto, buscamos
variables que tuvieran una gran cantidad de 0s (en un 75% o más de las observaciones), ya que este
tipo de resultados pueden deberse a que 1) el gen no se expresa o 2) ha habido algún error en el
método de cuantificación o falta de sensibilidad. En cualquier caso, este tipo de variables probablemente 
no van a aportar mucha información que ayude a clasificar las observaciones, por lo que se han descartado
en pos de tener una base de datos lo más manejable posible. Del mismo modo, se han buscado aquellas variables
que tuvieran el mismo valor en todas las observaciones (es decir, una desviación estándard == 0), porque 
tampoco hubieran podido aportar valor clasificatorio. No hemos encontrado ninguna variable que
cumpliese ese criterio, pero de haberlo hecho, también la hubieramos eliminado.


### Pregunta 2. Métodos no supervisados.
**¿Cuál es el motivo por el cual habéis seleccionado estas técnicas de reducción de dimensionalidad?**

Hemos decidido usar t-SNE y UMAP porque ambos pueden tratar con datos no lineales, y ambos
separan bien los grupos de forma visual (además, a partir de los ejemplos de clase y de la actividad 1 nos
parecían los mejores métodos). También son los métodos que más parecen usarse (algunos compañeros del grupo
los han visto en artículos y en sus trabajos), por lo que nos interesaba aprender a usarlos.

**¿Cuál es el motivo por el cual habéis seleccionado estas técnicas de clusterización?**

Hemos decidido usar un método jerarquico y uno no-jerárquico. Como de los no jerárquicos
sólo hemos visto k-Means en clase, ese es el que hemos hecho. De los jerárquicos decidimos hacer 
el divisivo (DIANA), porque en teoría es menos costoso que el jerarquico algomerativo.

**En ambos casos, ¿qué aspectos positivos y negativos tienen cada una?**


*tSNE:*

Positivos: captura bien relaciones complejas, preserva localidad de datos y también
aporta algo de la estructura global.

Negativos: tiene componente estocástico y por eso hay que usar seeds, se usa sobre todo
para visualizar porque el rendimiento con >3 dimensiones no es el mejor. Computacionalmente
costoso con grandes cantidades de datos.


*UMAP:*

Positivos: Es eficaz y versátil para reducir la dimensionalidad.

Negativos: Muy complejo debido a todos los parámetros personalizables, que a su vez impactan
sobre el coste computacional. Aunque en teoría tiene componente estocástico (como t-SNE), 
no se observan grandes variaciones.


*k-Means:*

Positivos: Es sencillo y poco costoso computacionalmente.

Negativos: A veces es dificil encontrar la k óptima, y es sensible a outliers.


*DIANA:*

Positivos: Menos costoso computacionalmente que otros métodos jerárquicos. A diferencia de los 
métodos agloerativos que van juntando puntos y se quedan unidos, los métodos divisivos empiezan
por un cluster y van separando, por lo que no se ven "obligados" a mantener puntos juntos; lo que los
hace más flexibles que los aglomerativos.

Negativos: Sensible a outliers. Puede ser difícil encontrar el número de clústeres óptimo y elegir
la métrica óptima para la clusterización (nosotros escogimos manhattan, pero no podemos asegurar
que sea la mejor opción).


**En el caso de la clusterización, ¿podéis afirmar con certeza que los clústeres generados son**
l**os mejores posibles? Razonad vuestra respuesta.**

En el caso del método k-Means seleccionamos el número de clusters basándonos en la
regla del codo del gráfico "clusters_plot". Podemos observar que después de 4 clusters
la pendiente disminuye, por lo que tener en cuenta más clusters a partir de 4 puede no 
darnos información importante. Si tuvieramos que elegir otro número de clusters sería 6, ya que
también se observa otra disminución de la pendiente tras ese número (en cambio, en el cluster 5 no).
No obstante, no creemos que valga la pena agregar dos clusters más por el poco extra de información
que puedan aportar. En resumen, creemos que en este caso los 4 clusters generados sí son los mejores.

En cambio, para la clusterización con DIANA, hemos elegido el número de clusters basándonos en el
dendograma en sí. En este caso, no tenemos tan claro que los clusters sean los mejores posibles. 
Por un lado se ven muchas diferencias intra-cluster, y por otro hay un cluster (lila) que realmente tiene
muy poca representación. Jugando con la k llegamos a ver que visualmente el dendograma quedaba más bonito
si generabamos 12 clústers. No obstante, nos parecía un número demasiado elevado, por lo que decidimos
dejarlo en 5 por ser el número de tipo de tumores. En resumen, en este caso no podemos afirmar
que sean los mejores clústeres.


### Pregunta 3. Métodos supervisados

**¿Cuál es el motivo por el cual habéis seleccionado ambas técnicas de aprendizaje supervisado?**
**¿Cuál ha dado mejores resultados a la hora de clasificar las muestras? Razonad vuestra respuesta**

Decidimos empezar con kNN por ser un método simple. No obstante, al ver sus resultados casi perfectos
decidimos usar un método más complejo (SVM radial) y después un método ensemble (Random Forest) para asegurarnos
de que no estuviera ocurriendo over-fitting en nuestro modelo.

De los tres métodos usados, los que han dado mejores resultados han sido el K-Nearest Neightbors y el
Random Forest (clasificaron bien / mal el mismo número de pacientes, y las distintas métricas fueron
idénticas).
Como ya mencionamos en medio del código, ambos métodos sólo clasificaron mal 1 paciente (un CFB
que realmente era AGH) por lo que las diferentes métricas evaluadas por clase (precisión, sensibilidad,
especificidad) eran perfectas para el resto de clases, mientras que para AGH y CFG eran algo
menores (aunque aún así casi perfectas). Decidimos hacer una tabla para comparar el score F1 de cada
modelo y llegamos a la misma conclusión: kNN y RF tenían los mejores scores, con valores perfectos para
CGC, CHC y HPB, y 0.98 y 0.99 para AGH y CFB, respectivamente).

**¿Habéis considerado oportuno implementar algún método de reducción de dimensionalidad**
**para procesar los datos antes de implementarlos en dichas técnicas? ¿Por qué?**

Como teniamos muchas variables y no estabamos seguros de que todas ellas fuesen a aportar información
para los métodos, decidimos primero hacer un Elastic Net con el propósito de eliminar aquellas
no tan necesárias. Elegimos Elastic Net en vez de Ridge o Lasso porque querímos eliminar variables para
simplificar el análisis, pero sin perder demasiadas.
Es por ello que decidimos eliminar de los análisis supervisados posteriores todas aquellas variables 
que tuvieran coeficiente 0 para todas las clases de tumores.


**¿Qué aspectos positivos y negativos tienen cada una de las técnicas que habéis escogido?**

*K-Nearest Neightbors (KNN):*

Positivos: Algoritmo simple y con capacidad adaptativa a nuevos datos introducidos. La fase 
de entrenamiento es rápida porque no se construye un modelo per se.

Negativos: La fase de clasificación puede ser lenta si hay muchos datos. No puede manejar datos
faltantes o que no tengan un preprocesamiento adecuado, y no trabaja bien con datos desequilibrados. 
Además la selección del mejor número de vecinos (k) a veces puede ser arbitraria.


*Support Vector Machine (SVM) de tipo kernel radial:*

Positivos: Como no se limita a hiperplanos lineales, puede separar grupos no lineales. Es eficaz con
altas dimensionalidades. Se pueden controlar parámetros para prevenir over-fitting.

Negativos: Se necesita una previa normalización de los datos para mejorar el modelo. Puede
ser difícil encontrar los parámetros óptimos.


*Random Forest (RF):*

Positivos: Al combinar multiples modelos, se reduce la varianza del modelo final y se mejora 
la precisión y el rendimiento (mejor para evitar over-fitting). Es menos sensible a outliers.

Negativos: Computacionalmente complejo. Como es el resultado de combinar varios modelos, su
interpretación puede ser compleja. Aunque este tipo de modelos ayuden con el over-fitting, si
los modelos base que se combinan presentan over-fitting, pueden pasárselo al modelo final.


### Pregunta 4. 
**De estas cuatro opciones, ¿qué tipo de arquitectura de deep learning sería **
**la más adecuada para procesar datos de expresión génica? Razonad vuestra respuesta.**

**a) Red de perceptrones (multiperceptron layers).**

**b) Redes convolucionales.**

**c) Redes recurrentes.**

**d) Redes de grafos.**


Para procesar datos de expresión génica, entendiéndolo como el tipo de datos con los 
que hemos estado trabajando durante esta actividad, podríamos descartar primero varias 
arquitecturas:

- Redes convolucionales: se usan con imágenes y vídeos, siendo capaces de reconocer patrones.
Por lo que, en nuestro caso no sería adecuado.

- Redes recurrentes: se usan para datos secuenciales de tipo texto o audio, por ejemplo para
autocompletar palabras que ya se han introducido antes. Tampoco aplicaría a nuestro ejemplo.

- Redes de grafos: podríamos usarlos si tuvieramos, por ejemplo, grafos
con la interacción que hay entre distintos genes. Pero como no es con lo que estamos
trabajando actualmente, no sería la más adecuada.

Por lo tanto, la arquitectura más adecuada en nuestro caso (teniendo una tabla típica de 
expresión de genes por muestra) seria la red de perceptrones, que consistiría en una capa 
de entrada (con tantas neuronas como genes estamos teniendo en cuenta, o menos si hemos 
hecho algun tipo de reducción de dimensionalidad previo), varias capas ocultas intermedias,
y una capa de salida que nos permetiría clasificar a los pacientes a partir de su expresión 
de genes (por ejemplo, en el caso en el que hemos estado trabajando en esta actividad, la 
capa de salida tendría 5 categorías, los 5 tipos de tumores).